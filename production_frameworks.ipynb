{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks for ML scaling and production\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple API with Flask and Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and pickle a model as `model.pkl`, then just create an app that accepts `POST` requests to the root path:\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "    from flask import Flask, jsonify, request\n",
    "    import pickle\n",
    "\n",
    "    # load model\n",
    "    model = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "    # app\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # routes\n",
    "    @app.route('/', methods=['POST'])\n",
    "\n",
    "    def predict():\n",
    "        # get data\n",
    "        data = request.get_json(force=True)\n",
    "\n",
    "        # convert data into dataframe\n",
    "        data.update((x, [y]) for x, y in data.items())\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "        # predictions\n",
    "        result = model.predict(data_df)\n",
    "\n",
    "        # send back to browser\n",
    "        output = {'results': int(result[0])}\n",
    "\n",
    "        # return data\n",
    "        return jsonify(results=output)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(port = 5000, debug=True)\n",
    "```\n",
    "\n",
    "Save the required packages to `requirements.txt`, where each line is of the format `package==version`. If using a clean environment, this can be done with:\n",
    ">`pip freeze > requirements.txt`\n",
    "\n",
    "To deploy to Heroku, create `Procfile` with the following contents:\n",
    ">`web: gunicorn app:app\n",
    "\n",
    "From within the Heroku web interface, Github repos can be deployed with a few clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "\n",
    "https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Use Case\n",
    "\n",
    "Hadoop is an open-source distributed data management system. It combines tools to store, analyze, and process large-scale pools of data on clusters of servers, without requiring specialized hardware. The \"vanilla\" version maintained by the Apache Foundation is quite intricate and not entirely stable, so there are many commercial distributions offered by third parties (such as Cloudera, Hortonworks). The major cloud services ([Google](https://cloud.google.com/dataproc?hl=en), Amazon, Microsoft) can also host Hadoop, either with their own out-of-the-box solutions or provided by commercial distributions.\n",
    "\n",
    "This [table](https://hadoopecosystemtable.github.io/) summarizes libraries and applications within the Hadoop \"ecosystem,\" including those produced by Apache itself and many others.\n",
    "\n",
    "Cloud data systems like Hadoop represent an alternative to relational databases in order to provide greater scalability and speed at large scales. It is often said that databases can optimize on 2 of 3 goals (CAP): consistency, availability, and partitioning (i.e. scalability). SQL priortizes C and A, while Hadoop prioritizes A and P. Because it lacks the transaction control of relational databases, it is better suited to \"behavioral\" rather than \"line of business\" data (such as customer accounts, supply chains, etc). Behavioral data is collected *in aggregate* as side-effect of user activity. Rather than being tracked and queried on the level of individuals, this data is primarily useful for the general patterns than can be seen in it -- hence it is acceptable to deprioritize consistency in a way that would not be workable for business-critical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives for Running Hadoop\n",
    "\n",
    "1. Apache Hadoop open source versus vendor services\n",
    "1. Docker images versus virtual machines\n",
    "1. Local file system, pseudo-distributed, fully distributed on own servers, versus on the cloud\n",
    "1. Versioning: Apache Hadoop updates frequently, and there are incompatbilities with some versions. MapReduce in particular went through a major 1.0 to 2.0 transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of the Hadoop Ecosystem\n",
    "\n",
    "### Hadoop File System (HDFS)\n",
    "\n",
    "Developed out of a system published by Google, HDFS promises scalability on \"commodity\" hardware. By default, it employs 3x data redundancy and enables larger chunk sizes than other formats. It is also possible to use the native file systems of cloud services.\n",
    "\n",
    "HDFS is immutable: any operations on data are saved as new files in the system rather than altering existing data. This includes re-executing operations: by default this will generate new outputs files every time instead of overwriting.\n",
    "\n",
    "The HDFS command-line interface syntax is `hadoop fs -command` (or sometimes `dfs`) where `command` shares many Linux shell commands like `cat`, `mkdir`, `ls` etc plus distinctive commands like `put` and `get` to moves file betweens HDFS and other storage (local/cloud). HDFS locations are written as urls `hdfs://...`\n",
    "\n",
    "### MapReduce\n",
    "\n",
    "The distributed processing framework for Hadoop. Implemented in Java, MapReduce processes (and anything else executing on a Hadoop server) are executed in Java virtual machines (JVM). Each process is a distinct VM that does not share state. The quirk this introduces is that although the syntax is object-oriented (being Java, everything is a class, in this case Static classes), the paradigm is much closer to functional programming, as each process can only take in data and output results without being able to reference the results of other parallel instances.\n",
    "\n",
    "There are now also APIs for languages more commonly used in data science like Python and R, as well as interfaces for other systems programming languages like C# and C++.\n",
    "\n",
    "The basic unit of a MapReduce routine is the **Job**, which is instanciated to carry out Map and Reduce operations on data. The **Map** functionality applies some set of operations *on each node* in the Hadoop cluster. It returns a set of key/value pairs. The **Reduce** functionality aggregates key/value pairs (on some subset of nodes) and returns a combined list, which is stored as a new file in the system. In between these two steps, the data (duplicated across nodes) is \"shuffled and sorted\" to processing nodes. For efficiency, it is possible to do a preliminary **Combine** stage on the original node, so as to increase the density of data that needs to be transferred across nodes for sorting and later reduction.\n",
    "\n",
    "So, for example, a basic word count operation -- producing a list of the unique words in a text and their corresponding counts -- the map function would turn the text into a list of words (each with 1 instance) and the reduce function would take look at each word and sum up the instances.\n",
    "\n",
    "It is considered good practice to subdivide tasks so that each routine performs only a single operation, and more complex operations are the result of chains of jobs. Pre-processing, for example, can be run as a \"map only\" job.\n",
    "\n",
    "Jobs are run by submitting them to the scheduler: this takes the form of indicating a `.jar` file and the class name to run as main, plus needed arguments like source and output locations. From the command line, the syntax is `hadoop jar filename.jar input output`.\n",
    "\n",
    "MapReduce 1.0 was limited because it could only process in batch and was not easy to customize. The 2.0 update allows more \"on-time\" operations and more intricate controls of how operations are carried out.\n",
    "\n",
    "### YARN\n",
    "\n",
    "Yet Another Resource ____: an abstraction layer added along with MapReduce 2.0 that allows a wider range of data processing on top of HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "An alternative for distributed data processing engine, which primarily operates in memory. See the notes on PySpark below.\n",
    "\n",
    "### HBase\n",
    "\n",
    "A wide-column, schema-on-read (NoSQL) database format that acts as a relatively accessible front-end to data stored in a Hadoop cluster.\n",
    "\n",
    "### Hive\n",
    "\n",
    "A query language interface for HBase, which acts as a MapReduce front-end, also known as HQL or H-SQL. The syntax is similar to SQL, but backend is fundamentally different. For one thing because it is a front-end for MapReduce (via often HBase), it is executing batch jobs on the cluster, which can take substantial time.\n",
    "\n",
    "`CREATE TABLE` commands pull requested fields from data into a wide table, then `SELECT...WHERE` commands can pull out specific records. NB, since the underlying data is not relational, `JOIN` statements are often impractical.\n",
    "\n",
    "### Pig\n",
    "\n",
    "A scripting tool for Hadoop, used especially for data input and cleaning (ETL: extract, transform, load). Its native language is called Pig Latin.\n",
    "\n",
    "### Oozie\n",
    "\n",
    "A workflow manager used to coordinate scripts from multiple libraries. Jobs are scripted using XML, so commercial GUIs are often used in practice.\n",
    "\n",
    "### Sqoop\n",
    "\n",
    "Command-line utility for transferring data between relation databases and Hadoop clusters. \n",
    "\n",
    "### ZooKeeper\n",
    "\n",
    "Centralized service for Hadoop configuration information, to create ensembles of programs. It performs computation in-memory for more real-time operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "PySpark is the Python API for Apache Spark. From the website:\n",
    "> Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.\n",
    "\n",
    "The Spark API is used to defined a graph of operations to be performed in parallel on a large, distributed dataset. The framework will try to optimize this for maximum parallelized efficiency. Accordingly, the methods of the Spark API are lazily evaluated. \n",
    "\n",
    "The connection with a Spark cluster through PySpark is managed by instances of the `SparkContext` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spark data structure\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so Spark provides a DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. They flatten out the nested complexity of the RDD structure, but unlike reading the data out directly into a Python object, they keep the operations within the Spark API.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in.\n",
    "\n",
    "### Sessions\n",
    "\n",
    "Within PySpark, the interface with data is encapsulated in `pyspark.sql.SparkSession` objects. To prevent conflicting coexisting sessions, the class method `builder.getOrCreate()` returns an existing session if it exists and only opens a new one if not.\n",
    "\n",
    "Direct interface with the database is held in the session atribute `catalog` (which returns a `Catalog` instance). E.g. `session.catalog.listTables()` will list the available tables in the current database. The `table(\"name\")` method returns a **PySpark DataFrame** of the requested table from the catalog.\n",
    "\n",
    "Alternatively, SQL queries can be made with `session.sql()`. Note that to print the contents, call the `show()` method (i.e. printing does not work). It can be converted to a Pandas DataFrame with the `toPandas()` method. Conversely, to create a Spark DataFrame from Pandas, use `session.createDataFrame(df)`. Or data can be read directly from csv with `session.read.csv(file_name, header=True)`. \n",
    "\n",
    "A DataFrame exists only in local memory. To add it to the database, use the DataFrame's `createTempView('name')` or `createOrReplaceTempView('name')` method (NB the former will throw an exception if a view with the given name already exists). \n",
    "\n",
    "### Manipulating Data\n",
    "\n",
    "PySpark DataFrames are immutable, so any mutating operation is actually creating a copy, which can then be assigned over the previous variable name.\n",
    "\n",
    "The columns of the DataFrame are accessible as attributes or by indexing. These are Column objects, and they have overloaded operators as with Pandas Series. An alias for display can be defined for a Column object with the `alias()` method (used for tables produced by `select()`). Note that Columns appear to be lazily evaluated, so e.g. exceptions based on types are only raised when a column is joined to a DataFrame. Moreover, Columns created by operations on Columns are linked to specific names and IDs, so they can be used with `select()` or `withColumn()` only if the calling DataFrame has a matching column (NB the ID is changed by column overwriting).\n",
    "\n",
    "To create a new DataFrame with an added or transformed column: `df.withColumn(\"column_name\", column)`, where `column` is a Column object. Rename a column with the `withColumnedRenamed('oldName', 'newName')`.\n",
    "\n",
    "Column data types can be changed with the Column object's `cast('type')` method.\n",
    "\n",
    "DataFrames can be filtered using the `filter()` method, which accepts either a query string (akin to what follows a `WHERE` SQL) or a boolean array (eg. `df.col > 0`). Similarly, the `select()` method returns the a DataFrame with columns specified as positional arguments, which can be either the column names as strings or as Column objects, allowing transformed columns. To use SQL syntax to transform columns, use `selectExpr()`, where each positional argument is a SQL-style column identifier (i.e. something separated by a comma in a `SELECT` statement).\n",
    "\n",
    "Operations like min, max, and count can be performed in `selectExpr()` or as methods of a `GroupedData` object. To do the latter, it is necessary to call `df.groupBy()`, even with no argument (thus observation is a \"group\"). Functions on columns are available in the `pyspark.sql.functions` module, e.g. `functions.stddev('colname')`, returning a Column that can be passed to the `agg()` method of a GroupedData object. NB, the aggregation methods return DataFrames not Columns, so need to use functions instead inside `agg()`. Note also the lazy evaluation of the function: the column name is only resolved when the Column is passed to `agg()`.\n",
    "\n",
    "Joins can be done with the method `join(other, on, how)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Machine Learning models are implemented in the `pyspark.ml` module. Different models have different APIs:\n",
    "1. Transformer models, which have a `transform()` method, which takes and returns a DataFrame, performing transformations on the column(s) identified in the constructor with `inputCol=''` or `inputCols=[]` and creating `outputCol` or `outputCols`.\n",
    "1. Estimator models that perform fitted transformation. The constructor takes `inputCol` (etc), and the object's `fit()` method takes a DataFrame and returns a Transformer. \n",
    "1. Predictor models that carry out machine learning. The constructor takes a *single* `featureCol` (a feature vector, which can be created with the `VectorAssembler()` transformer) and `labelCol`. Note that `featureCol` defaults to `'features'`. The `fit()` method takes a DataFrame and returns a Transformer. NB these objects do have a `predict()` method, but it takes a single feature vector.\n",
    "\n",
    "For example, to apply one-hot encoding to string values:\n",
    "```python\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "    indexer = StringIndexer(inputCol = 'string_column', outputCol = 'cat_column')\n",
    "    fit_indexer = indexer.fit(df)\n",
    "    indexed_df = fit_indexer.transform(df)\n",
    "    encoder = OneHoteEncoder(intputCol = 'cat_column', outpotCol = 'encoded_col')\n",
    "    fit_encoder = encoder.fit(indexed_df)\n",
    "    encoded_df = fit_encoder.transform(indexed_df)\n",
    "```\n",
    "Note that this does not precisely generate one column per value but instead creates a column containing a tuple that can be interpreted by the models.\n",
    "\n",
    "Estimators and transformers can be combined into a `pyspark.ml.Pipeline(stages=[])`, where the parameter is a list of model objects.\n",
    "\n",
    "### Model evaluation and tuning\n",
    "\n",
    "Train-test split can be created with the DataFrame's `randomSplit()` method, which takes a list of $n$ proportions and returns a tuple of $n$ DataFrames.\n",
    "\n",
    "Predictor models have built-in `evaluate()` methods (though it's not clear from the docs what the metric is), but custom evaluations are defined in the `ml.evaluate` module. \n",
    "\n",
    "To tune a logistic regression model with 5-fold cross-validation:\n",
    "```python\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "    \n",
    "    # Create the parameter grid\n",
    "    grid = tune.ParamGridBuilder()\n",
    "    grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "    grid = grid.build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    cv = tune.CrossValidator(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "\n",
    "    cv_results = cv.fit(train)\n",
    "    best_lr = cv_results.bestModel\n",
    "    \n",
    "    best_predictions = best_lr.transform(test)\n",
    "    print(evaluator.evaluate(best_predictions))\n",
    "```\n",
    "I'm not sure how well this would work with a pipeline, since it is not clear how to access underlying parameters within a pipeline. The DataCamp course recommended doing all transformations in a pipeline, then splitting the data, then doing cross-validation on the training data, but this would contaminate the fit. The Cloudera presentation splits beforehand and applies the \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerization\n",
    "\n",
    "In order to reproduce the environment for applications, it is often helpful to encapsulate them through virtualization. This can be done with various virtual environment tool, but *containers* add an additional step of portability. **Docker** is one popular containerzation tool, providing funtionality to reproduce and run containers, as well as hosting DockerHub as a repository for Docker images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "It is important to distinguish between:\n",
    "- **Dockerfile**: A Dockerfile is a text file that specifies how an image will be created.\n",
    "- **Docker Images**: Images are created by building a Dockerfile.\n",
    "- **Docker Containers**: Docker containers is the running instance of an image.\n",
    "\n",
    "### The Dockerfile\n",
    "\n",
    "```\n",
    "+------------+-----------------------------------------------------+\n",
    "| Command    | Description                                         |\n",
    "+------------+-----------------------------------------------------+\n",
    "| FROM       | The base Docker image for the Dockerfile.           |\n",
    "| LABEL      | Key-value pair for specifying image metadata.       |\n",
    "| RUN        | It execute commands on top of the current image as  |\n",
    "|              new layers.                                         |\n",
    "| COPY       | Copies files from the local machine to the          |\n",
    "|              container filesystem.                               |\n",
    "| EXPOSE     | Exposes runtime ports for the Docker container.     |\n",
    "| CMD        | Specifies the command to execute when running the   |   \n",
    "|              container. This command is overridden if another    |   \n",
    "|              command is specified at runtime.                    |\n",
    "| ENTRYPOINT | Specifies the command to execute when running the   |      \n",
    "|              container. Entrypoint commands are not overridden   |\n",
    "|              by a command specified at runtime.                  |\n",
    "| WORKDIR    | Set working directory of the container.             |\n",
    "| VOLUME     | Mount a volume from the local machine filesystem to | \n",
    "|              the Docker container.                               |\n",
    "| ARG        | Set Environment variable as a key-value pair when   |              \n",
    "|              building the image.                                 |\n",
    "| ENV        | Set Environment variable as a key-value pair that   | \n",
    "|              will be available in the container after building.  |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Docker Images\n",
    "\n",
    "The command `docker build -t <image-name>` builds an image from the `Dockerfile` in the current directory. Docker keeps a records of local images, which can be managed with commands:\n",
    "```\n",
    "+---------------------------------+--------------------------------+\n",
    "| Command                         | Description                    |\n",
    "+---------------------------------+--------------------------------+\n",
    "| docker images                   | List all images on the         |   \n",
    "|                                   machine.                       |\n",
    "| docker rmi [IMAGE_NAME]         | Remove the image with name     | \n",
    "|                                   IMAGE_NAME on the machine.     |\n",
    "| docker rmi $(docker images -q)  | Remove all images from the     | \n",
    "|                                   machine.                       |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Running Containers\n",
    "\n",
    "The syntax for running an container from an image is as follows:\n",
    "```bash\n",
    "docker run [-d -it --rm --name <CONTAINER_NAME> -p <host:container>] <IMAGE_NAME>\n",
    "```\n",
    "\n",
    "- `-d`: run the container in detached mode. This mode runs the container in the background.\n",
    "- `-it`: run in interactive mode, with a terminal session attached.\n",
    "- `--rm`: remove the container when it exits.\n",
    "- `--name`: specify a name for the container.\n",
    "- `-p`: port forwarding from host to the container (i.e. host: container).\n",
    "\n",
    "```\n",
    "+-------------------------------+----------------------------------+\n",
    "| Command                       | Description                      |\n",
    "+-------------------------------+----------------------------------+\n",
    "| docker ps                     | List all containers. Append -a   |\n",
    "|                                 to also list containers not      | \n",
    "|                                 running.                         |\n",
    "| docker stop [CONTAINER_ID]    | Gracefully stop the container    |                            \n",
    "|                                 with [CONTAINER_ID] on the       |   \n",
    "|                                 machine.                         |\n",
    "| docker kill [CONTAINER_ID]     | Forcefully stop the container    |\n",
    "|                                 with [CONTAINER_ID] on the       |                      \n",
    "|                                 machine.                         |\n",
    "| docker rm [CONTAINER_ID]      | Remove the container with        |   \n",
    "|                                 [CONTAINER_ID] from the machine. |\n",
    "| docker rm $(docker ps -a -q)  | Remove all containers from the   | \n",
    "|                                 machine.                         |\n",
    "+------------+-----------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### A simple script\n",
    "\n",
    "We create a simple script `date-script.sh`:\n",
    "```bash\n",
    "#! /bin/sh\n",
    "    DATE=\"$(date)\"\n",
    "    echo \"Todays date is $DATE\"\n",
    "```\n",
    "\n",
    "And a `Dockerfile`:\n",
    "```bash\n",
    "    # base image for building container\n",
    "    FROM docker.io/alpine\n",
    "    # add maintainer label\n",
    "    LABEL maintainer=\"mark.simon.cohen@gmail.com\"\n",
    "    # copy script from local machine to container filesystem\n",
    "    COPY date-script.sh /date-script.sh\n",
    "    # execute script\n",
    "    CMD sh date-script.sh\n",
    "```\n",
    "\n",
    "The Docker image will be built-off the Alpine Linux package. See https://hub.docker.com/_/alpine\n",
    "\n",
    "`docker build -t simple` followed by `docker run simple` will print the date.\n",
    "\n",
    "### Serve a Webpage on an nginx Web Server with Docker\n",
    "\n",
    "Create an `index.html` file, and then a `Dockerfile`:\n",
    "```bash\n",
    "    # base image for building container\n",
    "    FROM docker.io/nginx\n",
    "    # add maintainer label\n",
    "    LABEL maintainer=\"mark.simon.cohen@gmail.com\"\n",
    "    # copy html file from local machine to container filesystem\n",
    "    COPY html/index.html /usr/share/nginx/html\n",
    "    # port to expose to the container\n",
    "    EXPOSE 80\n",
    "```\n",
    "\n",
    "Note that 80 is the default port for receiving html requests. So, as a Web server, this container will be listening on port 80.\n",
    "\n",
    "Now, `docker build -t nginx-server` and:\n",
    "```\n",
    "    docker run -d -it -p 8081:80 nginx-server\n",
    "```\n",
    "\n",
    "Two points: this runs in the background, and instructs Docker to capture local port 8081 and forward it to port 80 inside the container. Docker will print the full local port to access it, and navigating a browser to that will bring up the index page. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
