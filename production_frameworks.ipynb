{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frameworks for ML scaling and production\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple API with Flask and Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and pickle a model as `model.pkl`, then just create an app that accepts `POST` requests to the root path:\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "    from flask import Flask, jsonify, request\n",
    "    import pickle\n",
    "\n",
    "    # load model\n",
    "    model = pickle.load(open('model.pkl','rb'))\n",
    "\n",
    "    # app\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    # routes\n",
    "    @app.route('/', methods=['POST'])\n",
    "\n",
    "    def predict():\n",
    "        # get data\n",
    "        data = request.get_json(force=True)\n",
    "\n",
    "        # convert data into dataframe\n",
    "        data.update((x, [y]) for x, y in data.items())\n",
    "        data_df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "        # predictions\n",
    "        result = model.predict(data_df)\n",
    "\n",
    "        # send back to browser\n",
    "        output = {'results': int(result[0])}\n",
    "\n",
    "        # return data\n",
    "        return jsonify(results=output)\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        app.run(port = 5000, debug=True)\n",
    "```\n",
    "\n",
    "Save the required packages to `requirements.txt`, where each line is of the format `package==version`. If using a clean environment, this can be done with:\n",
    ">`pip freeze > requirements.txt`\n",
    "\n",
    "To deploy to Heroku, create `Procfile` with the following contents:\n",
    ">`web: gunicorn app:app`\n",
    "\n",
    "From within the Heroku web interface, Github repos can be deployed with a few clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "\n",
    "https://hadoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Use Case\n",
    "\n",
    "Hadoop is an open-source distributed data management system. It combines tools to store, analyze, and process large-scale pools of data on clusters of servers, without requiring specialized hardware. The \"vanilla\" version maintained by the Apache Foundation is quite intricate and not entirely stable, so there are many commercial distributions offered by third parties (such as Cloudera, Hortonworks). The major cloud services ([Google](https://cloud.google.com/dataproc?hl=en), Amazon, Microsoft) can also host Hadoop, either with their own out-of-the-box solutions or provided by commercial distributions.\n",
    "\n",
    "This [table](https://hadoopecosystemtable.github.io/) summarizes libraries and applications within the Hadoop \"ecosystem,\" including those produced by Apache itself and many others.\n",
    "\n",
    "Cloud data systems like Hadoop represent an alternative to relational databases in order to provide greater scalability and speed at large scales. It is often said that databases can optimize on 2 of 3 goals (CAP): consistency, availability, and partitioning (i.e. scalability). SQL priortizes C and A, while Hadoop prioritizes A and P. Because it lacks the transaction control of relational databases, it is better suited to \"behavioral\" rather than \"line of business\" data (such as customer accounts, supply chains, etc). Behavioral data is collected *in aggregate* as side-effect of user activity. Rather than being tracked and queried on the level of individuals, this data is primarily useful for the general patterns than can be seen in it -- hence it is acceptable to deprioritize consistency in a way that would not be workable for business-critical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives for Running Hadoop\n",
    "\n",
    "1. Apache Hadoop open source versus vendor services\n",
    "1. Docker images versus virtual machines\n",
    "1. Local file system, pseudo-distributed, fully distributed on own servers, versus on the cloud\n",
    "1. Versioning: Apache Hadoop updates frequently, and there are incompatbilities with some versions. MapReduce in particular went through a major 1.0 to 2.0 transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of the Hadoop Ecosystem\n",
    "\n",
    "### Hadoop File System (HDFS)\n",
    "\n",
    "Developed out of a system published by Google, HDFS promises scalability on \"commodity\" hardware. By default, it employs 3x data redundancy and enables larger chunk sizes than other formats. It is also possible to use the native file systems of cloud services.\n",
    "\n",
    "HDFS is immutable: any operations on data are saved as new files in the system rather than altering existing data. This includes re-executing operations: by default this will generate new outputs files every time instead of overwriting.\n",
    "\n",
    "The HDFS command-line interface syntax is `hadoop fs -command` (or sometimes `dfs`) where `command` shares many Linux shell commands like `cat`, `mkdir`, `ls` etc plus distinctive commands like `put` and `get` to moves file betweens HDFS and other storage (local/cloud). HDFS locations are written as urls `hdfs://...`\n",
    "\n",
    "### MapReduce\n",
    "\n",
    "The distributed processing framework for Hadoop. Implemented in Java, MapReduce processes (and anything else executing on a Hadoop server) are executed in Java virtual machines (JVM). Each process is a distinct VM that does not share state. The quirk this introduces is that although the syntax is object-oriented (being Java, everything is a class, in this case Static classes), the paradigm is much closer to functional programming, as each process can only take in data and output results without being able to reference the results of other parallel instances.\n",
    "\n",
    "There are now also APIs for languages more commonly used in data science like Python and R, as well as interfaces for other systems programming languages like C# and C++.\n",
    "\n",
    "The basic unit of a MapReduce routine is the **Job**, which is instanciated to carry out Map and Reduce operations on data. The **Map** functionality applies some set of operations *on each node* in the Hadoop cluster. It returns a set of key/value pairs. The **Reduce** functionality aggregates key/value pairs (on some subset of nodes) and returns a combined list, which is stored as a new file in the system. In between these two steps, the data (duplicated across nodes) is \"shuffled and sorted\" to processing nodes. For efficiency, it is possible to do a preliminary **Combine** stage on the original node, so as to increase the density of data that needs to be transferred across nodes for sorting and later reduction.\n",
    "\n",
    "So, for example, a basic word count operation -- producing a list of the unique words in a text and their corresponding counts -- the map function would turn the text into a list of words (each with 1 instance) and the reduce function would take look at each word and sum up the instances.\n",
    "\n",
    "It is considered good practice to subdivide tasks so that each routine performs only a single operation, and more complex operations are the result of chains of jobs. Pre-processing, for example, can be run as a \"map only\" job.\n",
    "\n",
    "Jobs are run by submitting them to the scheduler: this takes the form of indicating a `.jar` file and the class name to run as main, plus needed arguments like source and output locations. From the command line, the syntax is `hadoop jar filename.jar input output`.\n",
    "\n",
    "MapReduce 1.0 was limited because it could only process in batch and was not easy to customize. The 2.0 update allows more \"on-time\" operations and more intricate controls of how operations are carried out.\n",
    "\n",
    "### YARN\n",
    "\n",
    "Yet Another Resource ____: an abstraction layer added along with MapReduce 2.0 that allows a wider range of data processing on top of HDFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "\n",
    "An alternative for distributed data processing engine, which primarily operates in memory. See the notes on PySpark below.\n",
    "\n",
    "### HBase\n",
    "\n",
    "A wide-column, schema-on-read (NoSQL) database format that acts as a relatively accessible front-end to data stored in a Hadoop cluster.\n",
    "\n",
    "### Hive\n",
    "\n",
    "A query language interface for HBase, which acts as a MapReduce front-end, also known as HQL or H-SQL. The syntax is similar to SQL, but backend is fundamentally different. For one thing because it is a front-end for MapReduce (via often HBase), it is executing batch jobs on the cluster, which can take substantial time.\n",
    "\n",
    "`CREATE TABLE` commands pull requested fields from data into a wide table, then `SELECT...WHERE` commands can pull out specific records. NB, since the underlying data is not relational, `JOIN` statements are often impractical.\n",
    "\n",
    "### Pig\n",
    "\n",
    "A scripting tool for Hadoop, used especially for data input and cleaning (ETL: extract, transform, load). Its native language is called Pig Latin.\n",
    "\n",
    "### Oozie\n",
    "\n",
    "A workflow manager used to coordinate scripts from multiple libraries. Jobs are scripted using XML, so commercial GUIs are often used in practice.\n",
    "\n",
    "### Sqoop\n",
    "\n",
    "Command-line utility for transferring data between relation databases and Hadoop clusters. \n",
    "\n",
    "### ZooKeeper\n",
    "\n",
    "Centralized service for Hadoop configuration information, to create ensembles of programs. It performs computation in-memory for more real-time operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "PySpark is the Python API for Apache Spark. From the website:\n",
    "> Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python, and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.\n",
    "\n",
    "The Spark API is used to defined a graph of operations to be performed in parallel on a large, distributed dataset. The framework will try to optimize this for maximum parallelized efficiency. Accordingly, the methods of the Spark API are lazily evaluated. \n",
    "\n",
    "Spark enables interactive programming through shells, in different language flavors: Scala, R, and Python. The PySpark API is also usable through a Python librariy. The connection with a Spark cluster through PySpark is managed by instances of the `SparkContext` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "With a SparkContext instances `sc`:\n",
    "- `sc.version` prints the version of Spark\n",
    "- `sc.pythonVer` print the version of Python being used by Spark\n",
    "- `sc.master` identifies the server to which the shell is connect (`local[*]` for a local connection)\n",
    "\n",
    "Loading data into a Spark instance:\n",
    "- `sc.parallelize(array_like)` converts the given data to an RDD\n",
    "- `sc.textFile(file_path)` reads the given file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data with redundancy across multiple nodes in the cluster. The partioning can be handled automatically by Spark or managed in certain ways when creating RDDs (with the methods above). RDD objects have a `getNumPartitions()` to see how many partitions are used.\n",
    "\n",
    "An RDD is structured as an array. It is common for the elements to consist of key-value pairs. These are known as pair RDDs, though they are not a distinct data type in the API. RDD methods interpret a series of two-item tuples as key-value pairs. Thus, a map that generates two-value tuples will produce a paired RDD.\n",
    "\n",
    "Operations performed on RDDs are divided into two broad categories: transformations that generate a new RDD and actions that have some other result (and often trigger evaluation of the transformation graph). Elementary transformation methods include:\n",
    "- `map(f)`\n",
    "- `filter(f)`\n",
    "- `flatMap(f)`: applies a function that returns multiple values and then flattens all of the results into a single array.\n",
    "- `union(other)`: combines two RDDs\n",
    "- `coalesce(n)`: reparallelizes the RDD into `n` partitions\n",
    "\n",
    "Additional transformations that operate only on pair RDDs (calling these methods on RDDs that are not comprised of 2-value tuples will raise an error **when evaluated**):\n",
    "- `reduceByKey(f)`: sequentially performs a 2->1 function the set of values sharing keys and generates a new pair RDD with the results\n",
    "- `groupByKey()`: generates a paired RDD where the values are a special iterable class containing all values for the key in the original data\n",
    "- `sortByKey(ascending=True)`\n",
    "- `join()`: by default, carries out an inner join, where the values are tuples of the values for shared keys in the component RDDs\n",
    "\n",
    "Action methods include:\n",
    "- `collect()`: executes the graph and returns the result *as a list* (at least in PySpark). Key-value pairs are expressed as tuples.\n",
    "- `take(N)`: returns an array of N elements drawn from RDD\n",
    "- `first()`: equivalent to `take(1)`\n",
    "- `count()`: returns number of entries\n",
    "- `reduce(f)`: as in `functools`, the function must take 2 values and return 1\n",
    "- `saveAsTextFile(dir)`: writes each partition to a separate text file in the given directory\n",
    "\n",
    "Pair-value-only actions:\n",
    "- `countByKey()`: returns the counts in a dictionary\n",
    "- `collectAsMap()`: executes the graphs returns all results as a dictionary\n",
    "\n",
    "Note that for the collect and ByKey actions, care must be taken to not request more data than can fit in memory.\n",
    "\n",
    "In general, though, RDDs are hard to work with directly, so Spark provides a DataFrame abstraction built on top of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs. They flatten out the nested complexity of the RDD structure, but unlike reading the data out directly into a Python object, they keep the operations within the Spark API.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in.\n",
    "\n",
    "#### SparkSession\n",
    "\n",
    "Within PySpark, the DataFrames interface is encapsulated in `pyspark.sql.SparkSession` objects. To prevent conflicting coexisting sessions, the class method `builder.getOrCreate()` returns an existing session if it exists and only opens a new one if not. The idiomatic name for the SparkSession is `spark`.\n",
    "\n",
    "DataFrames stored in the database are accessible through the `catalog` (which returns a `Catalog` instance). Thus `spark.catalog.listTables()` will list the available tables in the current database. The `table(\"name\")` method returns a **PySpark DataFrame** of the requested table from the catalog. Alternatively, SQL queries can be made with `spark.sql()`: these treat the catalog like a relational database, allowing the creation of custom DataFrames with `SELECT ... FROM ...` statements.\n",
    "\n",
    "#### Creating DataFrames\n",
    "\n",
    "One key difference between an RDD and a DataFrame is that the latter requires a schema. Thus, creating a DataFrame table from an RDD entails, at a minimum, providing column names: `df = spark.createDataFrame(rdd, schema=col_names)`. Spark will infer the data types. The schema of a DataFrame can be seen with the `printSchema()` method, and a list of column names is visible with the through `columns` attribute.\n",
    "\n",
    "DataFrames can also be created directly from local data. To convert a Pandas DataFrame to a Spark DataFrame, use `spark.createDataFrame(df)`. Or data can be read directly from csv with `spark.read.csv(file_name, header=True, inferSchema=True)`. Spark DataFrames can also be converted to Pandas DataFrame with the `toPandas()` method.\n",
    "\n",
    "A newly created DataFrame exists only in local memory. To add it to the database, use the DataFrame's `createTempView('name')` or `createOrReplaceTempView('name')` method (NB the former will throw an exception if a view with the given name already exists).\n",
    "\n",
    "#### Manipulating DataFrames\n",
    "\n",
    "Note that to print the contents a DataFrame, call the `show()` method (i.e. printing does not work); limit to `n` rows with `show(n)`.\n",
    "\n",
    "PySpark DataFrames are immutable, so any mutating operation is actually creating a copy, which can then be assigned over the previous variable name.\n",
    "\n",
    "The columns of the DataFrame are accessible as attributes or by indexing. These are Column objects, and they have overloaded operators as with Pandas Series. An alias for display can be defined for a Column object with the `alias()` method (used for tables produced by `select()`). Note that Column transformations, as with RDDs, are lazily evaluated, so e.g. exceptions based on types are only raised when a column is joined to a DataFrame. Moreover, Columns created by operations on Columns are linked to specific names and IDs, so they can be used with `select()` or `withColumn()` only if the calling DataFrame has a matching column (NB the ID is changed by column overwriting).\n",
    "\n",
    "To create a new DataFrame with an added or transformed column: `df.withColumn(\"column_name\", column)`, where `column` is a Column object. Rename a column with the `withColumnedRenamed('oldName', 'newName')`.\n",
    "\n",
    "Column data types can be changed with the Column object's `cast('type')` method.\n",
    "\n",
    "DataFrames can be filtered using the `filter()` method, which accepts either a query string (akin to what follows a `WHERE` SQL) or boolean operation on a column (eg. `df.col > 0`). Similarly, the `select(*cols)` method returns a DataFrame with the columns specified as positional arguments, which can be either the column names as strings or as Column objects, allowing transformed columns. To use SQL syntax to transform columns, use `selectExpr()`, where each positional argument is a SQL-style column identifier (i.e. something separated by a comma in a `SELECT` statement).\n",
    "\n",
    "DataFrames can be sorted with the method `orderBy(*cols)`. Duplicates can be removed by `dropDuplicates(*cols)` (an empty parameter set will drop only complete duplicates).\n",
    "\n",
    "The `describe()` method creates a DataFrame of summary statistics for all numerical columns (or a subset specific by positional arguments). In addition, operations like `min`, `max`, and `mean` can be performed in `selectExpr()` or as methods of a `GroupedData` object. To do the latter, it is necessary to call `df.groupBy()`, even with no argument (thus observation is a \"group\"). Functions on columns are available in the `pyspark.sql.functions` module, e.g. `functions.stddev('colname')`, returning a Column that can be passed to the `agg()` method of a GroupedData object. NB, the aggregation methods of grouped objects return DataFrames not Columns, so need to use functions instead inside `agg()`. Note also the lazy evaluation of the function: the column name is only resolved when the Column is passed to `agg()`.\n",
    "\n",
    "Joins can be done with the method `join(other, on, how)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "There are different ways of getting visualizations out of PySpark data\n",
    "\n",
    "#### Converting to Pandas\n",
    "\n",
    "By converting a PySpark DataFrame to Pandas using `toPandas()`, any MatPlotLib or Seaborn plotting method can be used. Recall that this requires fitting all of the data into one machine's methods.\n",
    "\n",
    "#### Pyspark_dist_explore\n",
    "\n",
    "This package implements some basic distribution visualization plots for Spark DataFrames\n",
    "- `hist(df)`\n",
    "- `distplot(df)`\n",
    "- `pandas_histogram(df)`\n",
    "\n",
    "#### HandySpark visualization methods\n",
    "\n",
    "HandySpark DataFrames add some additional functionality while preserving the distributed character of Spark DataFrames, including visualization methods like `hist()`. They can be created by the Spark DataFrame `toHandy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib\n",
    "\n",
    "Apache Spark includes a machine learning library MLlib, which implements common feature engineering and modeling techniques for distributed data, including classification, regression, clustering, and collaborative filtering (recommendation). Note that these each have their own submodules, which contain classes for models and common transformations.\n",
    "\n",
    "**NOTE: this library seems to be semi-deprecated in favor of the DataFrame-based `ml` module covered below.**\n",
    "\n",
    "#### Utilities and basic syntax\n",
    "- RDDs have a `randomSplit([training_share, testing_share])` method that takes a list of two fractions and returns two RDDs with randomly selected values in the given proportions.\n",
    "- Models are simulateously initialized and trained with class method `MODEL.train(data, **params)`.\n",
    "\n",
    "#### Feature engineering\n",
    "`pyspark.mllib.feature`\n",
    "- Tokenization by hasing values: `HashingTF(numFeatures=d)` returns sparse vectors of length `d`\n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "`pyspark.mllib.recommendation`\n",
    "- `Rating(user, product, rating)`: a class for capturing product rating observations\n",
    "- `ALS`: alternating least squares algoritm. Its `train()` method accepts parameters `rank` and `iterations`. The `predictAll()` method on the trained model takes data tuples of user and product and predicts ratings, returning Rating objects. Its accuracy can be tested by manually implementing a mean square error measure:\n",
    "```python\n",
    "    # Prepare ratings data\n",
    "    rates = rtest_data.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "    # Prepare predictions data\n",
    "    preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "    # Join the ratings data with predictions data\n",
    "    rates_and_preds = rates.join(preds)\n",
    "\n",
    "    # Calculate and print MSE\n",
    "    MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "```\n",
    "\n",
    "#### Classification\n",
    "`pyspark.mllib.classification`\n",
    "- `Vectors`: can be created as either `dense(data)` or `sparse(n, {i: value,...})`\n",
    "- `LabeledPoint(label, feature_vector)`\n",
    "- `LogisticRegressionWithLBFGS`: accepts data of `LabeledPoint`s. Its `predict()` methods accepts a feature vector and returns a `LabeledPoint` object.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "`pyspark.mllib.clustering`\n",
    "- `KMeans`: `train()` takes floating point vectors, and named arguments `k` and `maxIterations`. The model objects `clusterCenters` attribute returns list of arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML on DataFrames\n",
    "\n",
    "Machine Learning models on DataFrames are implemented in the `pyspark.ml` module. Different models have different APIs:\n",
    "1. Transformer models, which have a `transform()` method, which takes and returns a DataFrame, performing transformations on the column(s) identified in the constructor with `inputCol=''` or `inputCols=[]` and creating `outputCol` or `outputCols`.\n",
    "1. Estimator models that perform fitted transformation. The constructor takes `inputCol` (etc), and the object's `fit()` method takes a DataFrame and returns a Transformer. \n",
    "1. Predictor models that carry out machine learning. The constructor takes a *single* `featureCol` (a feature vector, which can be created with the `VectorAssembler()` transformer) and `labelCol`. Note that `featureCol` defaults to `'features'`. The `fit()` method takes a DataFrame and returns a Transformer. NB these objects do have a `predict()` method, but it takes a single feature vector.\n",
    "\n",
    "For example, to apply one-hot encoding to string values:\n",
    "```python\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "    indexer = StringIndexer(inputCol = 'string_column', outputCol = 'cat_column')\n",
    "    fit_indexer = indexer.fit(df)\n",
    "    indexed_df = fit_indexer.transform(df)\n",
    "    encoder = OneHoteEncoder(intputCol = 'cat_column', outpotCol = 'encoded_col')\n",
    "    fit_encoder = encoder.fit(indexed_df)\n",
    "    encoded_df = fit_encoder.transform(indexed_df)\n",
    "```\n",
    "Note that this does not precisely generate one column per value but instead creates a column containing a tuple that can be interpreted by the models.\n",
    "\n",
    "Estimators and transformers can be combined into a `pyspark.ml.Pipeline(stages=[])`, where the parameter is a list of model objects.\n",
    "\n",
    "#### Model evaluation and tuning\n",
    "\n",
    "Train-test split can be created with the DataFrame's `randomSplit()` method, which takes a list of $n$ proportions and returns a tuple of $n$ DataFrames.\n",
    "\n",
    "Predictor models have built-in `evaluate()` methods (though it's not clear from the docs what the metric is), but custom evaluations are defined in the `ml.evaluate` module. \n",
    "\n",
    "To tune a logistic regression model with 5-fold cross-validation:\n",
    "```python\n",
    "    from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "    \n",
    "    # Create the parameter grid\n",
    "    grid = tune.ParamGridBuilder()\n",
    "    grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "    grid = grid.build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    cv = tune.CrossValidator(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "\n",
    "    cv_results = cv.fit(train)\n",
    "    best_lr = cv_results.bestModel\n",
    "    \n",
    "    best_predictions = best_lr.transform(test)\n",
    "    print(evaluator.evaluate(best_predictions))\n",
    "```\n",
    "I'm not sure how well this would work with a pipeline, since it is not clear how to access underlying parameters within a pipeline. The DataCamp course recommended doing all transformations in a pipeline, then splitting the data, then doing cross-validation on the training data, but this would contaminate the fit. The Cloudera presentation splits beforehand and applies the pipeline to each part separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containerization\n",
    "\n",
    "In order to reproduce the environment for applications, it is often helpful to encapsulate them through virtualization. This can be done with various virtual environment tool, but *containers* add an additional step of portability. **Docker** is one popular containerzation tool, providing funtionality to reproduce and run containers, as well as hosting DockerHub as a repository for Docker images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to distinguish between:\n",
    "- **Dockerfile**: A Dockerfile is a text file that specifies how an image will be created.\n",
    "- **Docker Images**: Images are created by building a Dockerfile.\n",
    "- **Docker Containers**: Docker containers is the running instance of an image.\n",
    "\n",
    "### The Dockerfile\n",
    "\n",
    "```\n",
    "+------------+-----------------------------------------------------+\n",
    "| Command    | Description                                         |\n",
    "+------------+-----------------------------------------------------+\n",
    "| FROM       | The base Docker image for the Dockerfile.           |\n",
    "| LABEL      | Key-value pair for specifying image metadata.       |\n",
    "| RUN        | It execute commands on top of the current image as  |\n",
    "|              new layers.                                         |\n",
    "| COPY       | Copies files from the local machine to the          |\n",
    "|              container filesystem.                               |\n",
    "| EXPOSE     | Exposes runtime ports for the Docker container.     |\n",
    "| CMD        | Specifies the command to execute when running the   |   \n",
    "|              container. This command is overridden if another    |   \n",
    "|              command is specified at runtime.                    |\n",
    "| ENTRYPOINT | Specifies the command to execute when running the   |      \n",
    "|              container. Entrypoint commands are not overridden   |\n",
    "|              by a command specified at runtime.                  |\n",
    "| WORKDIR    | Set working directory of the container.             |\n",
    "| VOLUME     | Mount a volume from the local machine filesystem to | \n",
    "|              the Docker container.                               |\n",
    "| ARG        | Set Environment variable as a key-value pair when   |              \n",
    "|              building the image.                                 |\n",
    "| ENV        | Set Environment variable as a key-value pair that   | \n",
    "|              will be available in the container after building.  |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Docker Images\n",
    "\n",
    "The command `docker build -t <image-name>` builds an image from the `Dockerfile` in the current directory. Docker keeps a records of local images, which can be managed with commands:\n",
    "```\n",
    "+---------------------------------+--------------------------------+\n",
    "| Command                         | Description                    |\n",
    "+---------------------------------+--------------------------------+\n",
    "| docker images                   | List all images on the         |   \n",
    "|                                   machine.                       |\n",
    "| docker rmi [IMAGE_NAME]         | Remove the image with name     | \n",
    "|                                   IMAGE_NAME on the machine.     |\n",
    "| docker rmi $(docker images -q)  | Remove all images from the     | \n",
    "|                                   machine.                       |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Running Containers\n",
    "\n",
    "The syntax for running an container from an image is as follows:\n",
    "```bash\n",
    "docker run [-d -it --rm --name <CONTAINER_NAME> -p <host:container> -v <source:target>] <IMAGE_NAME>\n",
    "```\n",
    "\n",
    "- `-d`: run the container in detached mode. This mode runs the container in the background.\n",
    "- `-it`: run in interactive mode, with a terminal session attached.\n",
    "- `--rm`: remove the container when it exits.\n",
    "- `--name`: specify a name for the container.\n",
    "- `-p`: port forwarding from host to the container (i.e. host: container).\n",
    "- `-v`: mount a local directory into the indicated directory within the container. Any changes made on the drive will be reflected in the running container (as opposed to when a file is copied).\n",
    "\n",
    "```\n",
    "+-------------------------------+----------------------------------+\n",
    "| Command                       | Description                      |\n",
    "+-------------------------------+----------------------------------+\n",
    "| docker ps                     | List all containers. Append -a   |\n",
    "|                                 to also list containers not      | \n",
    "|                                 running.                         |\n",
    "| docker stop [CONTAINER_ID]    | Gracefully stop the container    |                            \n",
    "|                                 with [CONTAINER_ID] on the       |   \n",
    "|                                 machine.                         |\n",
    "| docker kill [CONTAINER_ID]     | Forcefully stop the container    |\n",
    "|                                 with [CONTAINER_ID] on the       |                      \n",
    "|                                 machine.                         |\n",
    "| docker rm [CONTAINER_ID]      | Remove the container with        |   \n",
    "|                                 [CONTAINER_ID] from the machine. |\n",
    "| docker rm $(docker ps -a -q)  | Remove all containers from the   | \n",
    "|                                 machine.                         |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "### Using DockerHub\n",
    "\n",
    "- To connect to DockerHub: `docker login`\n",
    "- To upload a local image: `docker push <image>`\n",
    "- Pulling an image DockerHub: `docker pull <image>`\n",
    "\n",
    "Note: image names on DockerHub have format `user/name`. It is good practice to mimic this format in local names for easy syncing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "#### A simple script\n",
    "\n",
    "We create a simple script `date-script.sh`:\n",
    "```bash\n",
    "#! /bin/sh\n",
    "    DATE=\"$(date)\"\n",
    "    echo \"Todays date is $DATE\"\n",
    "```\n",
    "\n",
    "And a `Dockerfile`:\n",
    "```bash\n",
    "    # base image for building container\n",
    "    FROM docker.io/alpine\n",
    "    # add maintainer label\n",
    "    LABEL maintainer=\"mark.simon.cohen@gmail.com\"\n",
    "    # copy script from local machine to container filesystem\n",
    "    COPY date-script.sh /date-script.sh\n",
    "    # execute script\n",
    "    CMD sh date-script.sh\n",
    "```\n",
    "\n",
    "The Docker image will be built-off the Alpine Linux package. See https://hub.docker.com/_/alpine\n",
    "\n",
    "`docker build -t simple .` followed by `docker run simple` will print the date.\n",
    "\n",
    "#### Serve a Webpage on an nginx Web Server with Docker\n",
    "\n",
    "Create an `index.html` file, and then a `Dockerfile`:\n",
    "```bash\n",
    "    # base image for building container\n",
    "    FROM docker.io/nginx\n",
    "    # add maintainer label\n",
    "    LABEL maintainer=\"mark.simon.cohen@gmail.com\"\n",
    "    # copy html file from local machine to container filesystem\n",
    "    COPY html/index.html /usr/share/nginx/html\n",
    "    # port to expose to the container\n",
    "    EXPOSE 80\n",
    "```\n",
    "\n",
    "Note that 80 is the default port for receiving html requests. So, as a Web server, this container will be listening on port 80.\n",
    "\n",
    "Now, `docker build -t nginx-server .` and:\n",
    "```\n",
    "    docker run -d -it -p 8081:80 nginx-server\n",
    "```\n",
    "\n",
    "Two points: this runs in the background, and instructs Docker to capture local port 8081 and forward it to port 80 inside the container. Run `docker ps` to see the status of the container, and navigate a browser to `localhost:8081` to access the server.\n",
    "\n",
    "Then, to stop the server, run `docker stop <ID>`, using the idea that is printed when the container is run.\n",
    "\n",
    "#### Downloading and running Jupyter's tensorflow container\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/tensorflow-notebook\n",
    "docker run --rm -p 8888:8888 jupyter/tensorflow-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes\n",
    "\n",
    "Kubernetes is a software system, developed by Google, that addresses the concerns of deploying, scaling and monitoring containers. Hence, it is called a container orchestrator. Examples of other container orchestrators in the wild are Docker Swarm, Mesos Marathon and Hashicorp Nomad.\n",
    "\n",
    "Google offers its own service for running Kubernetes, but other vendows (e.g. Amazon) offer alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Kubernetes\n",
    "- Horizontal auto-scaling: dynamically scales containers based on resource demands.\n",
    "- Self-healing: re-provisions failed nodes in response to health checks.\n",
    "- Load balancing: efficiently distributes requests between containers in a pod.\n",
    "- Rollbacks and updates: easily update or revert to a previous container deployment without causing application downtime.\n",
    "- DNS service discovery: Uses Domain Name System (DNS) to manage container groups as a Kubernetes service.\n",
    "### Components of Kubernetes\n",
    "\n",
    "The main components of the Kubernetes engine are the:\n",
    "- Master node(s): manages the Kubernetes cluster. They may be more than one master node in High Availability mode for fault-tolerance purposes. In this case, only one is the master, and the others follow. Master nodes can contain the following functions:\n",
    "    - etcd (distributed key-store): manages the Kubernetes cluster state. This distributed key-store can be a part of the Master node or external to it. Nevertheless, all master nodes connect to it.\n",
    "    - api server: manages all administrative tasks. The api server receives commands from the user (kubectl cli,REST or GUI), these commands are executed and the new cluster state is stored in the distributed key-store.\n",
    "    - scheduler: schedules work to worker nodes by allocating pods. It is responsible for resource allocation.\n",
    "    - controller: ensure that the desired state of the Kubernetes cluster is maintained. The desired state is what is contained in a JSON or YAML deployment file.\n",
    "- Worker node(s): machine(s) that runs containerized applications that are scheduled as pod(s). Each worker node is comprised of the following:\n",
    "    - kubelet: the kubelet agent runs on each worker node. It connects the worker node to the api server on the master node and received instructions from it. Ensures the pods on the node are healthy.\n",
    "    - kube-proxy: it is the Kubernetes network proxy that runs on each worker node. It listens to the api server and forward requests to the appropriate pod. Important for load-balancing.\n",
    "    - pod(s): consists of one or more containers that share network and storage resources as well as container runtime instructions. Pods are the smallest deployable unit in Kubernetes.\n",
    "\n",
    "### Configuring and Deploying a Kubernetes cluster\n",
    "\n",
    "Kubernetes is controlled by a deployment file in `yaml` format. This specifies the objects and specifications that should be deployed. `kubectl` provides a command-line interface\n",
    "\n",
    "```\n",
    "+-------------------------------------------+----------------------+\n",
    "| Command                                   | Description          |\n",
    "+-------------------------------------------+----------------------+\n",
    "| kubectl get all                           | list all resources.  |\n",
    "| kubectl get pods                          | list pods.           |                            \n",
    "| kubectl get service                       | list services.       | \n",
    "| kubectl get deployments --all-namespaces  | list deployments for | \n",
    "|                                             all namespaces.      | \n",
    "| kubectl create -f [DEPLOYMENT_FILE.yaml]  | create a new resource|  \n",
    "|                                             based on the desired | \n",
    "|                                             state in the yaml    |  \n",
    "|                                             file.                | \n",
    "| kubectl apply -f [DEPLOYMENT_FILE.yaml]   | if the resource      |  \n",
    "|                                             already exists,      | \n",
    "|                                             refresh the resource |  \n",
    "|                                             based on the yaml.   |             \n",
    "|                                             file.                |\n",
    "| kubectl delete -f [DEPLOYMENT_FILE.yaml]  | remove all resources |  \n",
    "|                                             from the yaml file.  |\n",
    "| kubectl get nodes                         | get the nodes of the | \n",
    "|                                             Kubernetes cluster.  | \n",
    "| kubectl delete deployment [DEPLOYMENT_NAME] | delete the         | \n",
    "|                                               deployment with    | \n",
    "|                                               [DEPLOYMENT_NAME]. |\n",
    "| kubectl delete svc [SERVICE_NAME]         | delete the service   | \n",
    "|                                             with [SERVICE_NAME]. |\n",
    "| kubectl delete pod [POD_NAME]             | delete the pod with  | \n",
    "|                                             [POD_NAME].          |\n",
    "+------------+-----------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Kubernetes locally with Minikube\n",
    "\n",
    "https://kubernetes.io/docs/tasks/tools/install-minikube/\n",
    "\n",
    "```\n",
    "+---------------------+--------------------------------------------+\n",
    "| Command             | Description                                |\n",
    "+---------------------+--------------------------------------------+\n",
    "| minikube status     | Check if Minikube is running.              |\n",
    "| minikube start      | Create local kubernetes cluster.           |                            \n",
    "| minikube stop       | Stop a running local kubernetes cluster.   |\n",
    "| minikube dashboard  | Open Minikube GUI for interacting with the | \n",
    "|                       Kubernetes cluster. Append & to open in    | \n",
    "|                       background mode minikube dashboard &.      |\n",
    "| minikube ip         | get ip address of Kubernetes cluster.      |\n",
    "+------------+-----------------------------------------------------+\n",
    "```\n",
    "\n",
    "After starting the cluster use `kubectl` to deploy a docker image. When done, delete the service and then stop Minikube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow\n",
    "\n",
    "A set of tools for management the deployment of machine learning workflows on Kubernetes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "springboard-data-science-KDNDKsui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
